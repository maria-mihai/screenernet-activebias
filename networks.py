from typing import Callable, Sequence, Any

import flax
import flax.linen as nn

import jax
import jax.numpy as jnp


class NonLinearMVN(nn.Module):
    """Multivariate Normal distribution with parameters generated by a given module"""
    projection: nn.Module
    
    scale_init: jax.typing.ArrayLike = 1.0  # Can also be an Array.

    @nn.compact
    def __call__(self, *args) -> tuple[jax.Array, jax.Array]:
        
        flat = jax.tree_map(jnp.ravel, args)
        stacked = jnp.concatenate(flat, -1)
        
        out = self.projection(stacked)

        mean, stddev = jnp.split(out, 2)
        stddev = jnp.clip(jax.nn.softplus(1.0 + stddev) + 1e-4, a_max=5.0)
        
        return mean, stddev


class MeanAggregator(nn.Module):
    """Arithmetic mean-aggregation of a set of input-output pairs into an output-distribution"""
    likelihood: NonLinearMVN
    
    @nn.compact
    def __call__(self, inputs: jax.Array) -> tuple[jax.Array, jax.Array]:
        
        aggregated = inputs.mean(axis=0)
        
        return self.likelihood(aggregated)


class SequenceAggregator(nn.Module):
    """Sequential rnn-aggregation of a set of input-output pairs into an output-distribution"""
    likelihood: NonLinearMVN
    num_hidden: int = 128
    
    @nn.compact
    def __call__(self, inputs: jax.Array) -> tuple[jax.Array, jax.Array]:
        lstm = nn.RNN(nn.LSTMCell(self.num_hidden))

        out = lstm(inputs[None, ...])

        last_hidden = out.at[0, -1, :].get()

        return self.likelihood(last_hidden)


class MLP(nn.Module):
    """Helper module for a MultiLayer-Perceptron with optional LayerNorm.
    
    # Example use-case for predicting binary output:
    model = MLP([32, 16, 1], activate_final=True, activation=jax.nn.sigmoid)

    # or
    model = MLP([32, 16, 1],
        activate_final=False, activation=jax.nn.relu, use_layernorm=True
    )
    """
    
    layer_features: Sequence[int]
    dense_kwargs: dict[str, Any] | None = None

    activation: Callable[[jax.Array, ...], jax.Array] = nn.leaky_relu
    activate_final: bool = False

    use_layernorm: bool = False

    @nn.compact
    def __call__(self, x: jax.Array) -> jax.Array:

        if self.use_layernorm:

            layers = [
                l for size in self.layer_features for l in (
                    nn.Dense(size, **(self.dense_kwargs or {})),
                    nn.LayerNorm(), self.activation
                )
            ]

            if not self.activate_final:
                layers.pop(); layers.pop()

        else:
            layers = [
                l for size in self.layer_features for l in (
                    nn.Dense(size, **(self.dense_kwargs or {})),
                    self.activation
                )
            ]

            if not self.activate_final:
                layers.pop()

        out = nn.Sequential(layers)(x)
        
        return out


class ResBlock(nn.Module):
    """Helper to create a residual layer from another module.

    Transformation is computed as: 
        Resblock(x): out = activation(module(x) - x)

    Optionally also applies LayerNorm to `out`.
    """
    module: nn.Module
    activation: Callable[[jax.Array, ...], jax.Array] = nn.leaky_relu
    norm: bool = True

    @nn.compact
    def __call__(self, x: jax.Array) -> jax.Array:

        output = self.module(x)
        out = self.activation(output - x)

        if self.norm:
            out = nn.LayerNorm()(out)

        return out


class MixtureNeuralProcess(nn.Module):
    """Implements the Neural-Process model for general mixture density models.
    
    See: 
     - Neural Processes. Gernelo et al., 2018. https://arxiv.org/abs/1807.01622
     - Mixture Density Networks. Bishop C., 1994. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf
    """
    embed_xs: nn.Module
    embed_ys: nn.Module
    embed_both: nn.Module
    
    posterior_fun: nn.Module
    likelihood: NonLinearMVN
    
    def posterior(self, X: jax.Array, y: jax.Array) -> jax.Array:
        """Map a set of context (X, y) points into the posterior-distribution p(Z | X, Y)"""
        
        embed_xs = nn.vmap(
            lambda m, *a: m(*a),
            variable_axes={'params': None},
            split_rngs={'params': False},
            in_axes=0
        )(self.embed_xs, X)
        embed_ys = nn.vmap(
            lambda m, *a: m(*a),
            variable_axes={'params': None},
            split_rngs={'params': False},
            in_axes=0
        )(self.embed_ys, y)
        combined = jnp.concatenate([embed_xs, embed_ys], axis=-1)
        
        embedded = nn.vmap(
            lambda m, *a: m(*a),
            variable_axes={'params': None},
            split_rngs={'params': False},
            in_axes=0
        )(self.embed_both, combined)
        return self.posterior_fun(embedded)

    def predictive(self, Zs: jax.Array, xs: jax.Array) -> tuple[jax.Array, jax.Array]:
        """Construct the posterior predictive distribution p(Y | Z, X) over samples Z and X.

        i.e., this function computes for each x_i in xs:
            p(Y | Z, x_i) \approx sum_i p(Y | z_j, x_i), z_j \sim p(Z | X-context, Y-context).

        See self.posterior for p(Z | X-context, Y-context).
        """
        xs_embed = nn.vmap(
            lambda m, *a: m(*a),
            variable_axes={'params': None},
            split_rngs={'params': False},
            in_axes=0
        )(self.embed_xs, xs)
        
        # Batches firstly over `Zs` keeping xs fixed, then batches over `xs` keeping Z fixed.
        mean, stddev = nn.vmap(
            nn.vmap(
                lambda m, *a: m(*a),
                variable_axes={'params': None},
                split_rngs={'params': False},
                in_axes=(None, 0)
            ),
            variable_axes={'params': None},
            split_rngs={'params': False},
            in_axes=(0, None), out_axes=1
        )(self.likelihood, Zs, xs_embed)
        
        return mean, stddev
    
    def elbo(
        self, 
        X: jax.Array, 
        y: jax.Array, 
        x_test: jax.Array, 
        y_test: jax.Array, 
        k: int = 10,
        beta: float = 1.0
    ) -> jax.Array:
        """Evidence lower-bound on the data-marginal likelihood p(X, Y).

        This objective should be maximized w.r.t. the model parameters to amortize inference.

        Short Derivation:

            p(Y | X) = \int p(Y | X, z) p(z | X) dz

        assume, p(Z | X) = p(Z). Then do importance sampling with variational model q(Z | X, Y).

            \ln p(Y | X) \ge \int q(z | X, Y) \ln [p(Y | X, z) p(z) / q(z | X, Y) ] dz
                         = E_q \ln p(Y | X, Z) - KL(q || p)

        inference is then amortized (trained) by optimizing the parameters of q to maximize the last term.

        Sampling Z_i from q, we get a Gaussian mixtures for the predictive, 
            
            \ln p(Y | X, Z_i) = \ln \sum w_i N(Y; \mu(X, Z_i), \var(X, Z_i))

        where we assume uniform weights w_i = 1/Num_Z. 
        
        Using the Gaussian log-likelihood, we compute the mixture likelihood as,
        
            \ln p(Y | X, Z) = logsumexp(-ln Num_Z + ln N(Y; \mu_i(X, Z), \var_i(X, Z)))
        
        See for more details: 
         - Neural Processes. Gernelo et al., 2018. https://arxiv.org/abs/1807.01622
         - Mixture Density Networks. Bishop C., 1994. https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf
        """
        
        full_X = jnp.concatenate([X, x_test], axis=0)
        full_Y = jnp.concatenate([y, y_test], axis=0)

        # NP Posterior
        mu_z, scale_z = self.posterior(full_X, full_Y)
        
        # NP Prior (treated as a constant in the optimization)
        mu_prior, scale_prior = self.posterior(X, y)
        mu_prior, scale_prior = jax.tree_map(jax.lax.stop_gradient, (mu_prior, scale_prior))
        scale_prior = scale_prior + 1.0

        # Monte-Carlo on NP Posterior
        epsilon = jax.random.normal(self.make_rng(), shape=(k, len(mu_z)))
        zs = mu_z + scale_z * epsilon

        # Combine latent and deterministic path
        zs = jnp.concatenate([zs, jnp.broadcast_to(mu_z, zs.shape)], -1)

        # Predictive of dim: 2 x (len(x_test), k, *dim(y_test))
        mu_out, scale_out = self.predictive(zs, x_test)

        # Compute empirical cross-entropy for each component k
        y_test = jnp.broadcast_to(
            y_test.reshape(len(y_test), 1, *y_test.shape[1:]),
            mu_out.shape
        )
        component_fit = -(jnp.log(scale_out * jnp.sqrt(2 * jnp.pi)) +
                          jnp.square((y_test - mu_out) / scale_out) / 2)
        component_fit = component_fit.sum(
            axis=tuple(range(2, len(y_test.shape) - 2))  # Excludes axis=(0, 1)
        )

        # Softmax over uniform mixture weights since they are posterior samples
        mixture_fit = jax.nn.logsumexp(-jnp.log(k) + component_fit, axis=1)

        log_likelihood = mixture_fit.mean()

        # KL(posterior || prior) for Diagonal MVN Gaussian
        kl_div = jnp.square((mu_prior - mu_z) / scale_prior).sum()
        kl_div = kl_div + jnp.square(scale_z / scale_prior).sum() - mu_z.size
        kl_div = kl_div + 2 * jnp.log(scale_prior).sum() - 2 * jnp.log(scale_z).sum()
        kl_div = kl_div / 2
        
        return log_likelihood - beta * kl_div

    def err(
            self,
            X: jax.Array,
            y: jax.Array,
            x_test: jax.Array,
            y_test: jax.Array,
            k: int = 10,
            beta: float = 1.0
    ) -> jax.Array:
        """
        Squared error / model variance
        """

        full_X = jnp.concatenate([X, x_test], axis=0)
        full_Y = jnp.concatenate([y, y_test], axis=0)

        # NP Posterior
        mu_z, scale_z = self.posterior(full_X, full_Y)

        # NP Prior (treated as a constant in the optimization)
        mu_prior, scale_prior = self.posterior(X, y)
        mu_prior, scale_prior = jax.tree_map(jax.lax.stop_gradient, (mu_prior, scale_prior))
        scale_prior = scale_prior + 1.0

        # Monte-Carlo on NP Posterior
        epsilon = jax.random.normal(self.make_rng(), shape=(k, len(mu_z)))
        zs = mu_z + scale_z * epsilon

        # Combine latent and deterministic path
        zs = jnp.concatenate([zs, jnp.broadcast_to(mu_z, zs.shape)], -1)

        # Predictive of dim: 2 x (len(x_test), k, *dim(y_test))
        mu_out, scale_out = self.predictive(zs, x_test)

        # Compute empirical cross-entropy for each component k
        y_test = jnp.broadcast_to(
            y_test.reshape(len(y_test), 1, *y_test.shape[1:]),
            mu_out.shape
        )
        err = (scale_out + jnp.abs((y_test - mu_out) / scale_out)).mean(axis=None)
        return err

    @nn.compact
    def __call__(self, X, y, x_test, k: int = 10) -> tuple[jax.Array, jax.Array]:
        """Joint call to p(Z | X, Y) and p(y | Z, x)"""

        # Posterior inference
        mu, scale = self.posterior(X, y)
        epsilon = jax.random.normal(self.make_rng(), shape=(k, len(mu)))
        samples = mu + scale * epsilon
        # Combine latent and deterministic path
        samples = jnp.concatenate([samples, jnp.broadcast_to(mu, samples.shape)], -1)
        # Posterior predictive
        mean, stddev = self.predictive(samples, x_test)
        return mean, stddev
